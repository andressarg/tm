{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicability and transparency in topic modelling: developing best practice guidelines for the digital humanities\n",
    "\n",
    "_Copyright (c) 2023 [Andressa Gomide, Mathew Gillings, Diego Gimenez]_\n",
    "\n",
    "This file is part of Gomide et al. 2023.\n",
    "\n",
    "This project is licensed under the terms of the MIT license.\n",
    "\n",
    "@get_gutemberg\n",
    "This script download books from the Gutemberg Project (https://www.gutenberg.org/), \n",
    "removes unecessary elements (e.g. boilerplates, page numbers),\n",
    "extract the metadata for each book\n",
    "and saves:\n",
    "- the original book file (html)\n",
    "- the cleaned content (txt)\n",
    "- the metadata (tsv)\n",
    "\n",
    "\n",
    "Functions in this file\n",
    "    * download_url - it takes an argument: \n",
    "    a string with with url path\n",
    "    and returns the content of the url as bytes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "if it doesn't require a lot of work, it might be a better idea to use your own code, as you have more awareness and avoid having a lot of dependencies. In this example, its better to use our own codes than importing https://pypi.org/project/Gutenberg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # for regular expressions\n",
    "from urllib.request import urlopen # to request the content from the internet\n",
    "from bs4 import BeautifulSoup # to work with html files (bs4 is known to be user friendly)\n",
    "import pandas as pd # to store metadata as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(urlpath):\n",
    "    ''' \n",
    "    download content from an url address\n",
    "    Args: \n",
    "        urlpath (str): the url path\n",
    "    Returns:\n",
    "        connection.read() (bytes): the content of the page \n",
    "    '''\n",
    "    try:\n",
    "        # open a connection to the server\n",
    "        with urlopen(urlpath, timeout=3) as connection:\n",
    "            # return content of the url read as bytes\n",
    "            return connection.read()\n",
    "    except:\n",
    "        # return None\n",
    "        print(f\"There was an issue when trying to download{urlpath}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a list with the books id.\n",
    "the id can be retrieved at https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_list = [\"44540\", \"55682\", \"31971\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sometimes the same data content is available in different formats. it is a good idea to test extracting two different formats to get an idea which one will be better for the project.\n",
    "- it is almost always easier to work with plain text, but preserving section breaks can lead to further analysis\n",
    "- in our case, getting the data from html format sounds better and easier to (a) preserve the sections boundaries (b) to make cleaning easier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting the books from the plain format\n",
    "\n",
    "we first create a data frame that will serve to store the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['author', 'title', 'lang', 'subj', 'datepub'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we go through the list of book ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_id in book_id_list:\n",
    "    # url for plain text book\n",
    "    url_plain = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt'\n",
    "\n",
    "    # download the content\n",
    "    data_plain = download_url(url_plain)\n",
    "\n",
    "    # plain text link doesnt include metadata. \n",
    "    # we have to go to the previous page\n",
    "    url_meta = f'https://www.gutenberg.org/ebooks/{book_id}'\n",
    "    metadata = download_url(url_meta)\n",
    "\n",
    "    # parse document \n",
    "    soup = BeautifulSoup(metadata, 'html.parser')\n",
    "\n",
    "    # get metadata\n",
    "    author = soup.find('a', {'about': re.compile(r'\\/authors\\/.*')}).text\n",
    "    lang = soup.find('a', {'href': re.compile(r'\\/browse\\/languages\\/.*')}).text\n",
    "    subj = soup.find('a', {'href': re.compile(r'\\/ebooks\\/subject\\/*')}).text\n",
    "    title = soup.find('td', {'itemprop': 'headline'}).text\n",
    "    datepub = soup.find('td', {'itemprop': 'datePublished'}).text\n",
    "\n",
    "    # remove line breaks\n",
    "    meta_list = [sub.replace('\\n', '') for sub in [author, title, lang, subj, datepub]]\n",
    "\n",
    "\n",
    "    # df.loc[book_id] = [book_id, meta_list[0], meta_list[1], meta_list[2], meta_list[3], meta_list[4]]\n",
    "    df.loc[book_id] = [meta_list[0], meta_list[1], meta_list[2], meta_list[3], meta_list[4]]\n",
    "\n",
    "    # write book content to file\n",
    "    with open(f\"input/{book_id}.txt\", 'wb') as file:\n",
    "        file.write(data_plain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and save the metadata as a tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       author  \\\n",
      "44540  Alencar, José Martiniano de, 1829-1877   \n",
      "55682             Machado de Assis, 1839-1908   \n",
      "31971              Queirós, Eça de, 1845-1900   \n",
      "\n",
      "                                               title        lang  \\\n",
      "44540                                  Cinco minutos  Portuguese   \n",
      "55682                                  Quincas Borba  Portuguese   \n",
      "31971  O crime do padre Amaro, scenas da vida devota  Portuguese   \n",
      "\n",
      "                                                    subj       datepub  \n",
      "44540                                            Fiction  Dec 29, 2013  \n",
      "55682  Brazil -- History -- Empire, 1822-1889 -- Fiction   Oct 5, 2017  \n",
      "31971                                Portugal -- Fiction  Apr 13, 2010  \n"
     ]
    }
   ],
   "source": [
    "# see the data\n",
    "print(df)\n",
    "\n",
    "# write metadata to file\n",
    "df.to_csv('output/books_metadata.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to get the books from html\n",
    "# create empty df to store the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           author  \\\n",
      "44540  José Martiniano de Alencar   \n",
      "55682                          NA   \n",
      "31971              Eça de Queirós   \n",
      "\n",
      "                                               title lang  \\\n",
      "44540                                  Cinco minutos   pt   \n",
      "55682                                  Quincas Borba   pt   \n",
      "31971  O crime do padre Amaro, scenas da vida devota   pt   \n",
      "\n",
      "                                                    subj     datepub  \n",
      "44540                                            Fiction  2013-12-29  \n",
      "55682  Brazil -- History -- Empire, 1822-1889 -- Fiction  2017-10-05  \n",
      "31971                                Portugal -- Fiction  2010-04-13  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['author', 'title', 'lang', 'subj', 'datepub'])\n",
    "\n",
    "for book_id in book_id_list:\n",
    "    url_html = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}-images.html'\n",
    "    data_html = download_url(url_html)\n",
    "\n",
    "    # parse\n",
    "    soup = BeautifulSoup(data_html, 'html.parser')\n",
    "\n",
    "    # get metadata\n",
    "    author = soup.find('meta', {'name' : 'AUTHOR'})['content'] if soup.find('meta', {'name' : 'AUTHOR'}) is not None else 'NA'\n",
    "    lang = soup.find('meta', {'name' : 'dc.language'})['content'] if soup.find('meta', {'name' : 'dc.language'}) is not None else 'NA'\n",
    "    subj = soup.find('meta', {'name' : 'dc.subject'})['content'] if soup.find('meta', {'name' : 'dc.subject'}) is not None else 'NA'\n",
    "    title = soup.find('meta', {'property' : 'og:title'})['content'] if soup.find('meta', {'property' : 'og:title'}) is not None else 'NA'\n",
    "    datepub = soup.find('meta', {'name' : 'dcterms.created'})['content'] if soup.find('meta', {'name' : 'dcterms.created'}) is not None else 'NA'\n",
    "\n",
    "    ## remove unnecessary elements\n",
    "    # style\n",
    "    for i in soup.find_all('style'):\n",
    "        i.decompose()\n",
    "\n",
    "    # boiler plates\n",
    "    for i in soup.find_all('section', {'class': re.compile('.*boilerplate.*')}):\n",
    "        i.decompose()\n",
    "\n",
    "    # editor comments\n",
    "    for i in soup.find_all('div', {'class': 'fbox'}):\n",
    "        i.decompose()\n",
    "\n",
    "    # page numbers\n",
    "    for i in soup.find_all('span', {'class': 'pagenum'}):\n",
    "        i.decompose()\n",
    "\n",
    "    # remove br tags\n",
    "    for i in soup.find_all('br'):\n",
    "        i.unwrap()\n",
    "\n",
    "    # remove head\n",
    "    soup.find('head').decompose()\n",
    "\n",
    "    # get metadata\n",
    "    df.loc[book_id] = [author, title, lang, subj, datepub]\n",
    "\n",
    "\n",
    "    # write to file with tags\n",
    "    with open(f'input/html/{book_id}.html', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(str(soup.prettify()))\n",
    "    # write to file without tags\n",
    "    with open(f'input/plain/{book_id}.txt', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(soup.text)\n",
    "\n",
    "print(df)\n",
    "# write metadata to file\n",
    "df.to_csv('output/books_metadata.tsv', sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cf00dd28cb843c250837a6461db654c95fdf5387681229a6a101a82db5d7e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
