{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicability and transparency in topic modelling: developing best practice guidelines for the digital humanities\n",
    "_Copyright (c) 2023 [Andressa Gomide, Mathew Gillings, Diego Gimenez]_\n",
    "\n",
    "This file is part of Gomide et al. 2023.\n",
    "\n",
    "This project is licensed under the terms of the MIT license.\n",
    "\n",
    "\n",
    "@create_bow\n",
    "This script\n",
    "- reads plain text files in a give folder\n",
    "- applies Spacy Lang model\n",
    "- creates different bags of words ('all_tokens', 'full_clean', 'custom_tok')\n",
    "and saves:\n",
    "- the original book file (html)\n",
    "- the cleaned content (txt)\n",
    "- the metadata (tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import spacy # to tokenize and annotate the data\n",
    "import pandas as pd # to store metadata as dataframe\n",
    "from gensim.models import Phrases # to compute the bigrams\n",
    "import utilsNLP # our library with functions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load language model\n",
    "\n",
    "there are different models availables at https://spacy.io/models \n",
    "\n",
    "we can also create our own\n",
    "\n",
    "here we will use a small model to be more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get list with files\n",
    "the folder input has the plain files prepared with @get_gutemberg.py\n",
    "the function get_file_list creates a list and append the files names to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_list = utilsNLP.get_file_list('input/plain')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "list of elements to be removed (we can also here our own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags to be removed\n",
    "pos_rm = ['PUNCT', 'DET', 'SPACE', 'NUM', 'SYM']\n",
    "# Named Entities tags to be removed\n",
    "ner_rm = ['PER', 'LOC']\n",
    "# words to be removed\n",
    "wrd_rm = ['ella', 'elle']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go through the files extracting the words and save the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting all tokens BoW...\n",
      "getting BoW with a 'full clean' approach ...\n",
      "getting customized BoW\n",
      "getting all tokens BoW...\n",
      "getting BoW with a 'full clean' approach ...\n",
      "getting customized BoW\n",
      "getting all tokens BoW...\n",
      "getting BoW with a 'full clean' approach ...\n",
      "getting customized BoW\n"
     ]
    }
   ],
   "source": [
    "# create empty df to store the different bag of words (BoWs)\n",
    "df = pd.DataFrame(columns = ['all_tokens', 'full_clean', 'custom_tok'])\n",
    "\n",
    "# iterate each file and create the 3 different BoWs\n",
    "for val in file_list:\n",
    "    # read file\n",
    "    with open(val, 'r', encoding='utf-8') as f:\n",
    "        text_org = f.read()\n",
    "    \n",
    "    # remove line breaks\n",
    "    text_oneline = text_org.replace(\"\\n\", \" \")\n",
    "\n",
    "    # apply model\n",
    "    nlp_text = nlp(text_org)\n",
    "\n",
    "    # create a list to store the NER labes to be \n",
    "    ne2rm = []\n",
    "    for ent in nlp_text.ents:\n",
    "        if ent.label_ in ner_rm:\n",
    "            ne2rm.append(ent.text.lower())\n",
    "\n",
    "    # get lis of unique values for the ner found\n",
    "    ne2rm = list(set(ne2rm))\n",
    "\n",
    "    # other possibilities\n",
    "    # - remove numbers, but not words that contain numbers...\n",
    "    # - Remove words that are only one character...\n",
    "\n",
    "    # all tokens (no space)\n",
    "    print(f'getting all tokens BoW for {val.stem}...')\n",
    "    all_tokens = [token.text.lower() for token in nlp_text if token.pos_ != 'SPACE']\n",
    "\n",
    "    # get all lemma that are not in the removel list neither in the stop list and that is alpha (not letters)\n",
    "    print(\"getting BoW with a 'full clean' approach ...\")\n",
    "    full_clean = [token.lemma_.lower() for token in nlp_text if token.pos_ not in pos_rm and not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # remove locations and named person/family\n",
    "    print(\"getting customized BoW\")\n",
    "    custom_tok = [token.text.lower() for token in nlp_text if token.text.lower() not in ne2rm and token.text.lower() not in wrd_rm and token.pos_ not in pos_rm and not token.is_stop]\n",
    "\n",
    "    # add BoWs to dataframe\n",
    "    df.loc[val.stem] = [all_tokens, full_clean, custom_tok]\n",
    "\n",
    "# write dataframe to file\n",
    "df.to_csv('output/bows.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# print df \n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute bigrams.\n",
    "as this can be a very heavy (and slow) process, we make it separately \n",
    "and save it in a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get only the values from the all_tokens column\n",
    "bow = df['all_tokens']\n",
    "\n",
    "len(bow[0]) # 93208\n",
    "len(df['all_tokens'][0]) # 1369489\n",
    "\n",
    "# get bigrams that occur at least 5 times\n",
    "bigrams = Phrases(bow, min_count=5)\n",
    "\n",
    "# add bigrams to BoW\n",
    "for idx in range(len(bow)):\n",
    "    for token in bigrams[bow[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            bow[idx].append(token)\n",
    "\n",
    "# save to file\n",
    "bow.to_csv('output/bow_with2gram.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cf00dd28cb843c250837a6461db654c95fdf5387681229a6a101a82db5d7e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
