{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicability and transparency in topic modelling: developing best practice guidelines for the digital humanities\n",
    "\n",
    "_Copyright (c) 2023 [Andressa Gomide, Mathew Gillings, Diego Gimenez]_\n",
    "\n",
    "This file is part of Gomide et al. 2023.\n",
    "\n",
    "This project is licensed under the terms of the MIT license.\n",
    "\n",
    "@get_gutemberg\n",
    "This script download books from the Gutemberg Project (https://www.gutenberg.org/), \n",
    "removes unecessary elements (e.g. boilerplates, page numbers),\n",
    "extract the metadata for each book\n",
    "and saves:\n",
    "- the original book file (html)\n",
    "- the cleaned content (txt)\n",
    "- the metadata (tsv)\n",
    "\n",
    "\n",
    "Functions in this file\n",
    "    * download_url - it takes an argument: \n",
    "    a string with with url path\n",
    "    and returns the content of the url as bytes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "if it doesn't require a lot of work, it might be a better idea to use your own code, as you have more awareness and avoid having a lot of dependencies. In this example, its better to use our own codes than importing https://pypi.org/project/Gutenberg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # for regular expressions\n",
    "from urllib.request import urlopen # to request the content from the internet\n",
    "from bs4 import BeautifulSoup # to work with html files (bs4 is known to be user friendly)\n",
    "import pandas as pd # to store metadata as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(urlpath):\n",
    "    ''' \n",
    "    download content from an url address\n",
    "    Args: \n",
    "        urlpath (str): the url path\n",
    "    Returns:\n",
    "        connection.read() (bytes): the content of the page \n",
    "    '''\n",
    "    try:\n",
    "        # open a connection to the server\n",
    "        with urlopen(urlpath, timeout=3) as connection:\n",
    "            # return content of the url read as bytes\n",
    "            return connection.read()\n",
    "    except:\n",
    "        # return None\n",
    "        print(f\"There was an issue when trying to download{urlpath}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a list with the books id.\n",
    "the id can be retrieved at https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_list = [\"44540\", \"55682\", \"31971\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sometimes the same data content is available in different formats. it is a good idea to test extracting two different formats to get an idea which one will be better for the project.\n",
    "- it is almost always easier to work with plain text, but preserving section breaks can lead to further analysis\n",
    "- in our case, getting the data from html format sounds better and easier to (a) preserve the sections boundaries (b) to make cleaning easier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting the books from the plain format\n",
    "\n",
    "we first create a data frame that will serve to store the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['author', 'title', 'lang', 'subj', 'datepub'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we go through the list of book ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_id in book_id_list:\n",
    "    # url for plain text book\n",
    "    url_plain = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt'\n",
    "\n",
    "    # download the content\n",
    "    data_plain = download_url(url_plain)\n",
    "\n",
    "    # plain text link doesnt include metadata. \n",
    "    # we have to go to the previous page\n",
    "    url_meta = f'https://www.gutenberg.org/ebooks/{book_id}'\n",
    "    metadata = download_url(url_meta)\n",
    "\n",
    "    # parse document \n",
    "    soup = BeautifulSoup(metadata, 'html.parser')\n",
    "\n",
    "    # get metadata\n",
    "    author = soup.find('a', {'about': re.compile(r'\\/authors\\/.*')}).text\n",
    "    lang = soup.find('a', {'href': re.compile(r'\\/browse\\/languages\\/.*')}).text\n",
    "    subj = soup.find('a', {'href': re.compile(r'\\/ebooks\\/subject\\/*')}).text\n",
    "    title = soup.find('td', {'itemprop': 'headline'}).text\n",
    "    datepub = soup.find('td', {'itemprop': 'datePublished'}).text\n",
    "\n",
    "    # remove line breaks\n",
    "    meta_list = [sub.replace('\\n', '') for sub in [author, title, lang, subj, datepub]]\n",
    "\n",
    "\n",
    "    # df.loc[book_id] = [book_id, meta_list[0], meta_list[1], meta_list[2], meta_list[3], meta_list[4]]\n",
    "    df.loc[book_id] = [meta_list[0], meta_list[1], meta_list[2], meta_list[3], meta_list[4]]\n",
    "\n",
    "    # write book content to file\n",
    "    with open(f\"input/{book_id}.txt\", 'wb') as file:\n",
    "        file.write(data_plain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and save the metadata as a tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       author  \\\n",
      "44540  Alencar, José Martiniano de, 1829-1877   \n",
      "55682             Machado de Assis, 1839-1908   \n",
      "31971              Queirós, Eça de, 1845-1900   \n",
      "\n",
      "                                               title        lang  \\\n",
      "44540                                  Cinco minutos  Portuguese   \n",
      "55682                                  Quincas Borba  Portuguese   \n",
      "31971  O crime do padre Amaro, scenas da vida devota  Portuguese   \n",
      "\n",
      "                                                    subj       datepub  \n",
      "44540                                            Fiction  Dec 29, 2013  \n",
      "55682  Brazil -- History -- Empire, 1822-1889 -- Fiction   Oct 5, 2017  \n",
      "31971                                Portugal -- Fiction  Apr 13, 2010  \n"
     ]
    }
   ],
   "source": [
    "# see the data\n",
    "print(df)\n",
    "\n",
    "# write metadata to file\n",
    "df.to_csv('output/books_metadata.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to get the books from html\n",
    "# create empty df to store the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           author  \\\n",
      "44540  José Martiniano de Alencar   \n",
      "55682                          NA   \n",
      "31971              Eça de Queirós   \n",
      "\n",
      "                                               title lang  \\\n",
      "44540                                  Cinco minutos   pt   \n",
      "55682                                  Quincas Borba   pt   \n",
      "31971  O crime do padre Amaro, scenas da vida devota   pt   \n",
      "\n",
      "                                                    subj     datepub  \n",
      "44540                                            Fiction  2013-12-29  \n",
      "55682  Brazil -- History -- Empire, 1822-1889 -- Fiction  2017-10-05  \n",
      "31971                                Portugal -- Fiction  2010-04-13  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['author', 'title', 'lang', 'subj', 'datepub'])\n",
    "\n",
    "for book_id in book_id_list:\n",
    "    url_html = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}-images.html'\n",
    "    data_html = download_url(url_html)\n",
    "\n",
    "    # parse\n",
    "    soup = BeautifulSoup(data_html, 'html.parser')\n",
    "\n",
    "    # get metadata\n",
    "    author = soup.find('meta', {'name' : 'AUTHOR'})['content'] if soup.find('meta', {'name' : 'AUTHOR'}) is not None else 'NA'\n",
    "    lang = soup.find('meta', {'name' : 'dc.language'})['content'] if soup.find('meta', {'name' : 'dc.language'}) is not None else 'NA'\n",
    "    subj = soup.find('meta', {'name' : 'dc.subject'})['content'] if soup.find('meta', {'name' : 'dc.subject'}) is not None else 'NA'\n",
    "    title = soup.find('meta', {'property' : 'og:title'})['content'] if soup.find('meta', {'property' : 'og:title'}) is not None else 'NA'\n",
    "    datepub = soup.find('meta', {'name' : 'dcterms.created'})['content'] if soup.find('meta', {'name' : 'dcterms.created'}) is not None else 'NA'\n",
    "\n",
    "    ## remove unnecessary elements\n",
    "    # style\n",
    "    for i in soup.find_all('style'):\n",
    "        i.decompose()\n",
    "\n",
    "    # boiler plates\n",
    "    for i in soup.find_all('section', {'class': re.compile('.*boilerplate.*')}):\n",
    "        i.decompose()\n",
    "\n",
    "    # editor comments\n",
    "    for i in soup.find_all('div', {'class': 'fbox'}):\n",
    "        i.decompose()\n",
    "\n",
    "    # page numbers\n",
    "    for i in soup.find_all('span', {'class': 'pagenum'}):\n",
    "        i.decompose()\n",
    "\n",
    "    # remove br tags\n",
    "    for i in soup.find_all('br'):\n",
    "        i.unwrap()\n",
    "\n",
    "    # remove head\n",
    "    soup.find('head').decompose()\n",
    "\n",
    "    # get metadata\n",
    "    df.loc[book_id] = [author, title, lang, subj, datepub]\n",
    "\n",
    "\n",
    "    # write to file with tags\n",
    "    with open(f'input/html/{book_id}.html', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(str(soup.prettify()))\n",
    "    # write to file without tags\n",
    "    with open(f'input/plain/{book_id}.txt', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(soup.text)\n",
    "\n",
    "print(df)\n",
    "# write metadata to file\n",
    "df.to_csv('output/books_metadata.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@create_bow\n",
    "This script\n",
    "- reads plain text files in a give folder\n",
    "- applies Spacy Lang model\n",
    "- creates different bags of words ('all_tokens', 'full_clean', 'custom_tok')\n",
    "and saves:\n",
    "- the original book file (html)\n",
    "- the cleaned content (txt)\n",
    "- the metadata (tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import spacy # to tokenize and annotate the data\n",
    "import pandas as pd # to store metadata as dataframe\n",
    "from gensim.models import Phrases # to compute the bigrams\n",
    "import utilsNLP # our library with functions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load language model\n",
    "\n",
    "there are different models availables at https://spacy.io/models \n",
    "\n",
    "we can also create our own\n",
    "\n",
    "here we will use a small model to be more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get list with files\n",
    "the folder input has the plain files prepared with @get_gutemberg.py\n",
    "the function get_file_list creates a list and append the files names to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_list = utilsNLP.get_file_list('input/plain')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "list of elements to be removed (we can also here our own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags to be removed\n",
    "pos_rm = ['PUNCT', 'DET', 'SPACE', 'NUM', 'SYM']\n",
    "# Named Entities tags to be removed\n",
    "ner_rm = ['PER', 'LOC']\n",
    "# words to be removed\n",
    "wrd_rm = ['ella', 'elle']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go through the files extracting the words and save the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty df to store the different bag of words (BoWs)\n",
    "df = pd.DataFrame(columns = ['all_tokens', 'full_clean', 'custom_tok'])\n",
    "\n",
    "# iterate each file and create the 3 different BoWs\n",
    "for val in file_list:\n",
    "    # read file\n",
    "    with open(val, 'r', encoding='utf-8') as f:\n",
    "        text_org = f.read()\n",
    "    \n",
    "    # remove line breaks\n",
    "    text_oneline = text_org.replace(\"\\n\", \" \")\n",
    "\n",
    "    # apply model\n",
    "    nlp_text = nlp(text_org)\n",
    "\n",
    "    # create a list to store the NER labes to be \n",
    "    ne2rm = []\n",
    "    for ent in nlp_text.ents:\n",
    "        if ent.label_ in ner_rm:\n",
    "            ne2rm.append(ent.text.lower())\n",
    "\n",
    "    # get lis of unique values for the ner found\n",
    "    ne2rm = list(set(ne2rm))\n",
    "\n",
    "    # other possibilities\n",
    "    # - remove numbers, but not words that contain numbers...\n",
    "    # - Remove words that are only one character...\n",
    "\n",
    "    # all tokens (no space)\n",
    "    print(f'getting all tokens BoW for {val.stem}...')\n",
    "    all_tokens = [token.text.lower() for token in nlp_text if token.pos_ != 'SPACE']\n",
    "\n",
    "    # get all lemma that are not in the removel list neither in the stop list and that is alpha (not letters)\n",
    "    print(\"getting BoW with a 'full clean' approach ...\")\n",
    "    full_clean = [token.lemma_.lower() for token in nlp_text if token.pos_ not in pos_rm and not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # remove locations and named person/family\n",
    "    print(\"getting customized BoW\")\n",
    "    custom_tok = [token.text.lower() for token in nlp_text if token.text.lower() not in ne2rm and token.text.lower() not in wrd_rm and token.pos_ not in pos_rm and not token.is_stop]\n",
    "\n",
    "    # add BoWs to dataframe\n",
    "    df.loc[val.stem] = [all_tokens, full_clean, custom_tok]\n",
    "\n",
    "# write dataframe to file\n",
    "df.to_csv('output/bows.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# print df \n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute bigrams.\n",
    "as this can be a very heavy (and slow) process, we make it separately \n",
    "and save it in a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get only the values from the all_tokens column\n",
    "bow = df['all_tokens']\n",
    "\n",
    "len(bow[0]) # 93208\n",
    "len(df['all_tokens'][0]) # 1369489\n",
    "\n",
    "# get bigrams that occur at least 5 times\n",
    "bigrams = Phrases(bow, min_count=5)\n",
    "\n",
    "# add bigrams to BoW\n",
    "for idx in range(len(bow)):\n",
    "    for token in bigrams[bow[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            bow[idx].append(token)\n",
    "\n",
    "# save to file\n",
    "bow.to_csv('output/bow_with2gram.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 12 2022, 19:14:26) [GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cf00dd28cb843c250837a6461db654c95fdf5387681229a6a101a82db5d7e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
