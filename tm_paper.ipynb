{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicability and transparency in topic modelling: developing best practice guidelines for the digital humanities\n",
    "\n",
    "_Copyright (c) 2023 [Andressa Gomide, Mathew Gillings, Diego Gimenez]_\n",
    "\n",
    "This file is part of Gomide et al. 2023.\n",
    "\n",
    "This project is licensed under the terms of the MIT license."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook is divided in three sections: (i) data collection and cleaning; (ii) tokenization, tagging and cleaning; and (iii) applying TM.\n",
    "Each section represents one file in the folder [link]\n",
    "\n",
    "### Data collection and cleaning\n",
    "@get_gutemberg.py\n",
    "The codes in this section are used to download books from the Gutemberg Project (https://www.gutenberg.org/); to remove unecessary elements (e.g. boilerplates, page numbers); to\n",
    "extract the metadata for each book; and to save the original book file (html), the cleaned content (txt), and the metadata (tsv)\n",
    "\n",
    "### Tokenization, Tagging and Cleaning\n",
    "@create_bow.py\n",
    "This section\n",
    "- reads plain text files in a give folder\n",
    "- applies Spacy Lang model\n",
    "- creates different bags of words ('all_tokens', 'full_clean', 'custom_tok')\n",
    "and saves:\n",
    "- the original book file (html)\n",
    "- the cleaned content (txt)\n",
    "- the metadata (tsv)\n",
    "\n",
    "### Applying TM\n",
    "@apply_tm.py\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in this file\n",
    "\n",
    " `download_url` - it takes a string with with url path as an argument and returns the content of the url as bytes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "- TODO add short explanation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For section 1\n",
    "import re # for regular expressions\n",
    "from urllib.request import urlopen # to request the content from the internet\n",
    "from bs4 import BeautifulSoup # to work with html files (bs4 is known to be user friendly)\n",
    "import pandas as pd # to store metadata as dataframe\n",
    "\n",
    "# For section 2\n",
    "import spacy # to tokenize and annotate the data\n",
    "import pandas as pd # to store metadata as dataframe\n",
    "from gensim.models import Phrases # to compute the bigrams\n",
    "import utilsNLP # our library with functions\n",
    "\n",
    "# For section 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Cleaning\n",
    "\n",
    "- keep original data (as obtained from source)\n",
    "- keep as much metadata as possible\n",
    "- if it doesn't require a lot of work, it might be a better idea to use your own code, as you have more awareness and avoid having a lot of dependencies. In this example, its better to use our own codes than importing https://pypi.org/project/Gutenberg/. In our case, to avoid unecessary repetition, we created the function `download_url` to get the content of the book from the website.\n",
    "- it is always easier to work with plain text, but preserving section breaks can lead to further analysis\n",
    "- sometimes the same data content is available in different formats. it is a good idea to test extracting two different formats to get an idea which one will be better for the project.\n",
    "- in our case, getting the data from html format sounds better and easier to (a) preserve the sections boundaries (b) to make cleaning easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(urlpath):\n",
    "    ''' \n",
    "    download content from an url address\n",
    "    Args: \n",
    "        urlpath (str): the url path\n",
    "    Returns:\n",
    "        connection.read() (bytes): the content of the page \n",
    "    '''\n",
    "    try:\n",
    "        # open a connection to the server\n",
    "        with urlopen(urlpath, timeout=3) as connection:\n",
    "            # return content of the url read as bytes\n",
    "            return connection.read()\n",
    "    except:\n",
    "        # return None\n",
    "        print(f\"There was an issue when trying to download{urlpath}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the books we want to download, we create a list with their identification numbers (IDs).\n",
    "The IDs can be retrieved at https://www.gutenberg.org/.\n",
    "Here we got all books by Machado de Assis in Portugese available at the Gutenberg project.\n",
    "\n",
    "54829   Memorias Posthumas de Braz Cubas\n",
    "55682   Quincas Borba\n",
    "55752   Dom Casmurro\n",
    "55797   Memorial de Ayres\n",
    "56737   Esau e Jacob\n",
    "57001   Papeis Avulsos\n",
    "67935   Reliquias de Casa Velha\n",
    "33056   Historias Sem Data\n",
    "53101   A Mao e A Luva\n",
    "67162   Helena\n",
    "67780   YayÃ¡ Garcia\n",
    "61653   Poesias Completas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_list = [\"54829\", \"55682\", \"55752\", \"55797\", \"56737\", \"57001\", \"67935\", \"33056\", \"53101\", \"67162\", \"67780\", \"61653\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sometimes the same data content is available in different formats. it is a good idea to test extracting two different formats to get an idea which one will be better for the project.\n",
    "- it is almost always easier to work with plain text, but preserving section breaks can lead to further analysis\n",
    "- in our case, getting the data from html format sounds better and easier to (a) preserve the sections boundaries (b) to make cleaning easier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the books from the plain format\n",
    "\n",
    "we first create a data frame that will serve to store the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['author', 'title', 'lang', 'subj', 'datepub'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we go through the list of book ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an issue when trying to downloadhttps://www.gutenberg.org/cache/epub/55797/pg55797.txt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# write book content to file\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput/\u001b[39m\u001b[39m{\u001b[39;00mbook_id\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m---> 32\u001b[0m     file\u001b[39m.\u001b[39;49mwrite(data_plain)\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "for book_id in book_id_list:\n",
    "    # url for plain text book\n",
    "    url_plain = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt'\n",
    "\n",
    "    # download the content\n",
    "    data_plain = download_url(url_plain)\n",
    "\n",
    "    # plain text link doesnt include metadata. \n",
    "    # we have to go to the previous page\n",
    "    url_meta = f'https://www.gutenberg.org/ebooks/{book_id}'\n",
    "    metadata = download_url(url_meta)\n",
    "\n",
    "    # parse document \n",
    "    soup = BeautifulSoup(metadata, 'html.parser')\n",
    "\n",
    "    # get metadata\n",
    "    author = soup.find('a', {'about': re.compile(r'\\/authors\\/.*')}).text\n",
    "    lang = soup.find('a', {'href': re.compile(r'\\/browse\\/languages\\/.*')}).text\n",
    "    subj = soup.find('a', {'href': re.compile(r'\\/ebooks\\/subject\\/*')}).text\n",
    "    title = soup.find('td', {'itemprop': 'headline'}).text\n",
    "    datepub = soup.find('td', {'itemprop': 'datePublished'}).text\n",
    "\n",
    "    # remove line breaks\n",
    "    meta_list = [sub.replace('\\n', '') for sub in [author, title, lang, subj, datepub]]\n",
    "\n",
    "\n",
    "    # df.loc[book_id] = [book_id, meta_list[0], meta_list[1], meta_list[2], meta_list[3], meta_list[4]]\n",
    "    df.loc[book_id] = [meta_list[0], meta_list[1], meta_list[2], meta_list[3], meta_list[4]]\n",
    "\n",
    "    # write book content to file\n",
    "    with open(f\"input/{book_id}.txt\", 'wb') as file:\n",
    "        file.write(data_plain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and save the metadata as a tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       author  \\\n",
      "44540  Alencar, JosÃ© Martiniano de, 1829-1877   \n",
      "55682             Machado de Assis, 1839-1908   \n",
      "31971              QueirÃ³s, EÃ§a de, 1845-1900   \n",
      "\n",
      "                                               title        lang  \\\n",
      "44540                                  Cinco minutos  Portuguese   \n",
      "55682                                  Quincas Borba  Portuguese   \n",
      "31971  O crime do padre Amaro, scenas da vida devota  Portuguese   \n",
      "\n",
      "                                                    subj       datepub  \n",
      "44540                                            Fiction  Dec 29, 2013  \n",
      "55682  Brazil -- History -- Empire, 1822-1889 -- Fiction   Oct 5, 2017  \n",
      "31971                                Portugal -- Fiction  Apr 13, 2010  \n"
     ]
    }
   ],
   "source": [
    "# see the data\n",
    "print(df)\n",
    "\n",
    "# write metadata to file\n",
    "df.to_csv('output/books_metadata.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to get the books from html\n",
    "# create empty df to store the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# page numbers\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mpagenum\u001b[39m\u001b[39m'\u001b[39m}):\n\u001b[0;32m---> 32\u001b[0m     i\u001b[39m.\u001b[39;49mdecompose()\n\u001b[1;32m     34\u001b[0m \u001b[39m# remove br tags\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mbr\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/coding/tm_paper/tmvenv/lib/python3.10/site-packages/bs4/element.py:1412\u001b[0m, in \u001b[0;36mTag.decompose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecompose\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1402\u001b[0m     \u001b[39m\"\"\"Recursively destroys this PageElement and its children.\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \n\u001b[1;32m   1404\u001b[0m \u001b[39m    This element will be removed from the tree and wiped out; so\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[39m    `decomposed` property.\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1412\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract()\n\u001b[1;32m   1413\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1414\u001b[0m     \u001b[39mwhile\u001b[39;00m i \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/coding/tm_paper/tmvenv/lib/python3.10/site-packages/bs4/element.py:361\u001b[0m, in \u001b[0;36mPageElement.extract\u001b[0;34m(self, _self_index)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m _self_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 361\u001b[0m         _self_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49mindex(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    362\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mcontents[_self_index]\n\u001b[1;32m    364\u001b[0m \u001b[39m#Find the two elements that would be next to each other if\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m#this element (and any children) hadn't been parsed. Connect\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m#the two.\u001b[39;00m\n",
      "File \u001b[0;32m~/coding/tm_paper/tmvenv/lib/python3.10/site-packages/bs4/element.py:1485\u001b[0m, in \u001b[0;36mTag.index\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[39m\"\"\"Find the index of a child by identity, not value.\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \n\u001b[1;32m   1479\u001b[0m \u001b[39mAvoids issues with tag.contents.index(element) getting the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[39m:param element: Look for this PageElement in `self.contents`.\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[39mfor\u001b[39;00m i, child \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontents):\n\u001b[0;32m-> 1485\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m element:\n\u001b[1;32m   1486\u001b[0m         \u001b[39mreturn\u001b[39;00m i\n\u001b[1;32m   1487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTag.index: element not in tag\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['author', 'title', 'lang', 'subj', 'datepub'])\n",
    "\n",
    "for book_id in book_id_list:\n",
    "    url_html = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}-images.html'\n",
    "    data_html = download_url(url_html)\n",
    "\n",
    "    # parse\n",
    "    soup = BeautifulSoup(data_html, 'html.parser')\n",
    "\n",
    "    # get metadata\n",
    "    author = soup.find('meta', {'name' : 'AUTHOR'})['content'] if soup.find('meta', {'name' : 'AUTHOR'}) is not None else 'NA'\n",
    "    lang = soup.find('meta', {'name' : 'dc.language'})['content'] if soup.find('meta', {'name' : 'dc.language'}) is not None else 'NA'\n",
    "    subj = soup.find('meta', {'name' : 'dc.subject'})['content'] if soup.find('meta', {'name' : 'dc.subject'}) is not None else 'NA'\n",
    "    title = soup.find('meta', {'property' : 'og:title'})['content'] if soup.find('meta', {'property' : 'og:title'}) is not None else 'NA'\n",
    "    datepub = soup.find('meta', {'name' : 'dcterms.created'})['content'] if soup.find('meta', {'name' : 'dcterms.created'}) is not None else 'NA'\n",
    "\n",
    "    ## remove unnecessary elements\n",
    "    # style\n",
    "    for i in soup.find_all('style'):\n",
    "        i.decompose()\n",
    "\n",
    "    # boiler plates\n",
    "    for i in soup.find_all('section', {'class': re.compile('.*boilerplate.*')}):\n",
    "        i.decompose()\n",
    "\n",
    "    # editor comments\n",
    "    for i in soup.find_all('div', {'class': 'fbox'}):\n",
    "        i.decompose()\n",
    "\n",
    "    # page numbers\n",
    "    for i in soup.find_all('span', {'class': 'pagenum'}):\n",
    "        i.decompose()\n",
    "\n",
    "    # remove br tags\n",
    "    for i in soup.find_all('br'):\n",
    "        i.unwrap()\n",
    "\n",
    "    # remove head\n",
    "    soup.find('head').decompose()\n",
    "\n",
    "    # get metadata\n",
    "    df.loc[book_id] = [author, title, lang, subj, datepub]\n",
    "\n",
    "\n",
    "    # write to file with tags\n",
    "    with open(f'input/html/{book_id}.html', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(str(soup.prettify()))\n",
    "    # write to file without tags\n",
    "    with open(f'input/plain/{book_id}.txt', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(soup.text)\n",
    "\n",
    "print(df)\n",
    "# write metadata to file\n",
    "df.to_csv('output/books_metadata.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@create_bow\n",
    "This script\n",
    "- reads plain text files in a give folder\n",
    "- applies Spacy Lang model\n",
    "- creates different bags of words ('all_tokens', 'full_clean', 'custom_tok')\n",
    "and saves:\n",
    "- the original book file (html)\n",
    "- the cleaned content (txt)\n",
    "- the metadata (tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load language model\n",
    "\n",
    "there are different models availables at https://spacy.io/models \n",
    "\n",
    "we can also create our own\n",
    "\n",
    "here we will use a small model to be more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get list with files\n",
    "the folder input has the plain files prepared with @get_gutemberg.py\n",
    "the function get_file_list creates a list and append the files names to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_list = utilsNLP.get_file_list('input/plain')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "list of elements to be removed (we can also here our own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags to be removed\n",
    "pos_rm = ['PUNCT', 'DET', 'SPACE', 'NUM', 'SYM']\n",
    "# Named Entities tags to be removed\n",
    "ner_rm = ['PER', 'LOC']\n",
    "# words to be removed\n",
    "wrd_rm = ['ella', 'elle']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go through the files extracting the words and save the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty df to store the different bag of words (BoWs)\n",
    "df = pd.DataFrame(columns = ['all_tokens', 'full_clean', 'custom_tok'])\n",
    "\n",
    "# iterate each file and create the 3 different BoWs\n",
    "for val in file_list:\n",
    "    # read file\n",
    "    with open(val, 'r', encoding='utf-8') as f:\n",
    "        text_org = f.read()\n",
    "    \n",
    "    # remove line breaks\n",
    "    text_oneline = text_org.replace(\"\\n\", \" \")\n",
    "\n",
    "    # apply model\n",
    "    nlp_text = nlp(text_org)\n",
    "\n",
    "    # create a list to store the NER labes to be \n",
    "    ne2rm = []\n",
    "    for ent in nlp_text.ents:\n",
    "        if ent.label_ in ner_rm:\n",
    "            ne2rm.append(ent.text.lower())\n",
    "\n",
    "    # get lis of unique values for the ner found\n",
    "    ne2rm = list(set(ne2rm))\n",
    "\n",
    "    # other possibilities\n",
    "    # - remove numbers, but not words that contain numbers...\n",
    "    # - Remove words that are only one character...\n",
    "\n",
    "    # all tokens (no space)\n",
    "    print(f'getting all tokens BoW for {val.stem}...')\n",
    "    all_tokens = [token.text.lower() for token in nlp_text if token.pos_ != 'SPACE']\n",
    "\n",
    "    # get all lemma that are not in the removel list neither in the stop list and that is alpha (not letters)\n",
    "    print(\"getting BoW with a 'full clean' approach ...\")\n",
    "    full_clean = [token.lemma_.lower() for token in nlp_text if token.pos_ not in pos_rm and not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # remove locations and named person/family\n",
    "    print(\"getting customized BoW\")\n",
    "    custom_tok = [token.text.lower() for token in nlp_text if token.text.lower() not in ne2rm and token.text.lower() not in wrd_rm and token.pos_ not in pos_rm and not token.is_stop]\n",
    "\n",
    "    # add BoWs to dataframe\n",
    "    df.loc[val.stem] = [all_tokens, full_clean, custom_tok]\n",
    "\n",
    "# write dataframe to file\n",
    "df.to_csv('output/bows.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# print df \n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute bigrams.\n",
    "as this can be a very heavy (and slow) process, we make it separately \n",
    "and save it in a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get only the values from the all_tokens column\n",
    "bow = df['all_tokens']\n",
    "\n",
    "len(bow[0]) # 93208\n",
    "len(df['all_tokens'][0]) # 1369489\n",
    "\n",
    "# get bigrams that occur at least 5 times\n",
    "bigrams = Phrases(bow, min_count=5)\n",
    "\n",
    "# add bigrams to BoW\n",
    "for idx in range(len(bow)):\n",
    "    for token in bigrams[bow[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            bow[idx].append(token)\n",
    "\n",
    "# save to file\n",
    "bow.to_csv('output/bow_with2gram.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cf00dd28cb843c250837a6461db654c95fdf5387681229a6a101a82db5d7e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
