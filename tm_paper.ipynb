{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicability and transparency in topic modelling: developing best practice guidelines for the digital humanities\n",
    "\n",
    "_Copyright (c) 2023 [Andressa Gomide, Mathew Gillings, Diego Gimenez]_\n",
    "\n",
    "This file is part of Gomide et al. 2023.\n",
    "\n",
    "This project is licensed under the terms of the MIT license."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook is divided in three sections: (i) data collection and cleaning; (ii) tokenization, tagging and cleaning; and (iii) applying TM.\n",
    "Each section represents one file in the folder [link]\n",
    "\n",
    "### Data collection and cleaning\n",
    "@get_gutemberg.py\n",
    "The codes in this section are used to download books from the Gutemberg Project (https://www.gutenberg.org/); to remove unecessary elements (e.g. boilerplates, page numbers); to\n",
    "extract the metadata for each book; and to save the original book file (html), the cleaned content (txt), and the metadata (tsv)\n",
    "\n",
    "### Tokenization, Tagging and Cleaning\n",
    "@create_bow.py\n",
    "This section\n",
    "- reads plain text files in a give folder\n",
    "- applies Spacy Lang model\n",
    "- creates different bags of words ('all_tokens', 'full_clean', 'custom_tok')\n",
    "and saves:\n",
    "- the original book file (html)\n",
    "- the cleaned content (txt)\n",
    "- the metadata (tsv)\n",
    "\n",
    "### Applying TM\n",
    "@apply_tm.py\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in this file\n",
    "\n",
    " `download_url` - it takes a string with with url path as an argument and returns the content of the url as bytes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "- TODO add short explanation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For section 1\n",
    "import re # for regular expressions\n",
    "from urllib.request import urlopen # to request the content from the internet\n",
    "from bs4 import BeautifulSoup # to work with html files (bs4 is known to be user friendly)\n",
    "import pandas as pd # to store metadata as dataframe\n",
    "from urllib.error import HTTPError # to raise errors when connecting to the sites\n",
    "\n",
    "# For section 2\n",
    "import spacy # to tokenize and annotate the data\n",
    "import pandas as pd # to store metadata as dataframe\n",
    "from gensim.models import Phrases # to compute the bigrams\n",
    "import utilsNLP # our library with functions\n",
    "\n",
    "# For section 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Cleaning\n",
    "\n",
    "- keep original data (as obtained from source)\n",
    "- keep as much metadata as possible\n",
    "- if it doesn't require a lot of work, it might be a better idea to use your own code, as you have more awareness and avoid having a lot of dependencies. In this example, its better to use our own codes than importing https://pypi.org/project/Gutenberg/. In our case, to avoid unecessary repetition, we created the function `download_url` to get the content of the book from the website.\n",
    "- it is always easier to work with plain text, but preserving section breaks can lead to further analysis\n",
    "- sometimes the same data content is available in different formats. it is a good idea to test extracting two different formats to get an idea which one will be better for the project.\n",
    "- in our case, getting the data from html format sounds better and easier to (a) preserve the sections boundaries (b) to make cleaning easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(urlpath):\n",
    "    ''' \n",
    "    download content from an url address\n",
    "    Args: \n",
    "        urlpath (str): the url path\n",
    "    Returns:\n",
    "        connection.read() (bytes): the content of the page \n",
    "    '''\n",
    "    try:\n",
    "        # open a connection to the server\n",
    "        with urlopen(urlpath, timeout=3) as connection:\n",
    "            # return content of the url read as bytes\n",
    "            return connection.read()\n",
    "    except:\n",
    "        # return None\n",
    "        print(f\"There was an issue when trying to download{urlpath}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the books we want to download, we create a list with their identification numbers (IDs).\n",
    "The IDs can be retrieved at https://www.gutenberg.org/.\n",
    "Here we got all books by Machado de Assis in Portugese available at the Gutenberg project.\n",
    "\n",
    "54829   Memorias Posthumas de Braz Cubas\n",
    "55682   Quincas Borba\n",
    "55752   Dom Casmurro\n",
    "55797   Memorial de Ayres\n",
    "56737   Esau e Jacob\n",
    "57001   Papeis Avulsos\n",
    "67935   Reliquias de Casa Velha\n",
    "33056   Historias Sem Data\n",
    "53101   A Mao e A Luva\n",
    "67162   Helena\n",
    "67780   Yayá Garcia\n",
    "61653   Poesias Completas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_list = [\"54829\", \"55682\", \"55752\", \"55797\", \"56737\", \"57001\", \"67935\", \"33056\", \"53101\", \"67162\", \"67780\", \"61653\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sometimes the same data content is available in different formats. it is a good idea to test extracting two different formats to get an idea which one will be better for the project.\n",
    "- it is almost always easier to work with plain text, but preserving section breaks can lead to further analysis\n",
    "- in our case, getting the data from html format sounds better and easier to (a) preserve the sections boundaries (b) to make cleaning easier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the books from the plain format\n",
    "\n",
    "we first create a data frame that will serve to store the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plain = pd.DataFrame(columns = ['author', 'title', 'lang', 'subj', 'datepub'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we go through the list of book ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading data for 54829, from link 1\n",
      "downloading data for 55682, from link 1\n",
      "downloading data for 55752, from link 1\n",
      "downloading data for 55797, from link 2\n",
      "downloading data for 56737, from link 1\n",
      "downloading data for 57001, from link 2\n",
      "downloading data for 67935, from link 1\n",
      "downloading data for 33056, from link 1\n",
      "downloading data for 53101, from link 1\n",
      "downloading data for 67162, from link 1\n",
      "downloading data for 67780, from link 1\n",
      "downloading data for 61653, from link 2\n"
     ]
    }
   ],
   "source": [
    "for book_id in book_id_list:\n",
    "    # # url for plain text book\n",
    "    # url_plain = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt'\n",
    "\n",
    "    # # download the content\n",
    "    # print(f'downloading content for {book_id}...')\n",
    "    # data_plain = download_url(url_plain)\n",
    "\n",
    "    # getting the content\n",
    "    try:\n",
    "        connection = urlopen(f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt')\n",
    "        data_plain = connection.read()\n",
    "        print(f'downloading data for {book_id}, from link 1')\n",
    "    except HTTPError as err:\n",
    "        if err.code == 404: # not found error (link doesnt exist)\n",
    "            connection = urlopen(f'https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt')\n",
    "            data_plain = connection.read()\n",
    "            print(f'downloading data for {book_id}, from link 2')\n",
    "        else:\n",
    "            print(f'error {err.code} when downloading file {book_id}')\n",
    "            continue\n",
    "\n",
    "    # plain text link doesnt include metadata. \n",
    "    # we have to go to the previous page\n",
    "    # TODO add try exept to metadata \n",
    "    # if it doesnt exist, add NA to the respective row\n",
    "    url_meta = f'https://www.gutenberg.org/ebooks/{book_id}'\n",
    "    metadata = download_url(url_meta)\n",
    "\n",
    "    # parse document \n",
    "    soup = BeautifulSoup(metadata, 'html.parser')\n",
    "\n",
    "    # get metadata\n",
    "    author = soup.find('a', {'about': re.compile(r'\\/authors\\/.*')}).text\n",
    "    lang = soup.find('a', {'href': re.compile(r'\\/browse\\/languages\\/.*')}).text\n",
    "    subj = soup.find('a', {'href': re.compile(r'\\/ebooks\\/subject\\/*')}).text\n",
    "    title = soup.find('td', {'itemprop': 'headline'}).text\n",
    "    datepub = soup.find('td', {'itemprop': 'datePublished'}).text\n",
    "\n",
    "    # remove line breaks\n",
    "    meta_list = [sub.replace('\\n', '') for sub in [author, title, lang, subj, datepub]]\n",
    "\n",
    "\n",
    "    # df.loc[book_id] = [book_id, meta_list[0], meta_list[1], meta_list[2], meta_list[3], meta_list[4]]\n",
    "    df_plain.loc[book_id] = [meta_list[0], meta_list[1], meta_list[2], meta_list[3], meta_list[4]]\n",
    "\n",
    "    # write book content to file\n",
    "    with open(f\"input/{book_id}.txt\", 'wb') as file:\n",
    "        file.write(data_plain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and save the metadata as a tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            author                             title  \\\n",
      "54829  Machado de Assis, 1839-1908  Memorias Posthumas de Braz Cubas   \n",
      "55682  Machado de Assis, 1839-1908                     Quincas Borba   \n",
      "55752  Machado de Assis, 1839-1908                      Dom Casmurro   \n",
      "55797  Machado de Assis, 1839-1908                 Memorial de Ayres   \n",
      "56737  Machado de Assis, 1839-1908                      Esau e Jacob   \n",
      "57001  Machado de Assis, 1839-1908                    Papeis Avulsos   \n",
      "67935  Machado de Assis, 1839-1908           Reliquias de Casa Velha   \n",
      "33056  Machado de Assis, 1839-1908                Historias Sem Data   \n",
      "53101  Machado de Assis, 1839-1908                    A Mao e A Luva   \n",
      "67162  Machado de Assis, 1839-1908                            Helena   \n",
      "67780  Machado de Assis, 1839-1908                       Yayá Garcia   \n",
      "61653  Machado de Assis, 1839-1908                 Poesias Completas   \n",
      "\n",
      "             lang                                               subj  \\\n",
      "54829  Portuguese                                   Humorous stories   \n",
      "55682  Portuguese  Brazil -- History -- Empire, 1822-1889 -- Fiction   \n",
      "55752  Portuguese                                Adultery -- Fiction   \n",
      "55797  Portuguese                                  Brazil -- Fiction   \n",
      "56737  Portuguese                 Rio de Janeiro (Brazil) -- Fiction   \n",
      "57001  Portuguese                 Portuguese fiction -- 19th century   \n",
      "67935  Portuguese                           Short stories, Brazilian   \n",
      "33056  Portuguese                                            Fiction   \n",
      "53101  Portuguese                                  Brazil -- Fiction   \n",
      "67162  Portuguese                  Brazilian fiction -- 19th century   \n",
      "67780  Portuguese                                  Brazil -- Fiction   \n",
      "61653  Portuguese                                   Brazilian poetry   \n",
      "\n",
      "            datepub  \n",
      "54829   Jun 2, 2017  \n",
      "55682   Oct 5, 2017  \n",
      "55752  Oct 15, 2017  \n",
      "55797  Oct 23, 2017  \n",
      "56737  Mar 14, 2018  \n",
      "57001  Apr 19, 2018  \n",
      "67935  Apr 26, 2022  \n",
      "33056   Jul 3, 2010  \n",
      "53101  Sep 20, 2016  \n",
      "67162  Jan 14, 2022  \n",
      "67780   Apr 5, 2022  \n",
      "61653  Mar 21, 2020  \n"
     ]
    }
   ],
   "source": [
    "# see the data\n",
    "print(df_plain)\n",
    "\n",
    "# write metadata to file\n",
    "df_plain.to_csv('output/books_metadata.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the books from html format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      author                             title lang  \\\n",
      "54829     NA  Memorias Posthumas de Braz Cubas   pt   \n",
      "55682     NA                     Quincas Borba   pt   \n",
      "55752     NA                      Dom Casmurro   pt   \n",
      "55797     NA                 Memorial de Ayres   pt   \n",
      "56737     NA                      Esau e Jacob   pt   \n",
      "57001     NA                    Papeis Avulsos   pt   \n",
      "67935     NA           Reliquias de Casa Velha   pt   \n",
      "33056     NA                Historias Sem Data   pt   \n",
      "53101     NA                    A Mao e A Luva   pt   \n",
      "67162     NA                            Helena   pt   \n",
      "67780     NA                       Yayá Garcia   pt   \n",
      "61653     NA                 Poesias Completas   pt   \n",
      "\n",
      "                                                    subj     datepub  \n",
      "54829                                   Humorous stories  2017-06-02  \n",
      "55682  Brazil -- History -- Empire, 1822-1889 -- Fiction  2017-10-05  \n",
      "55752                                Adultery -- Fiction  2017-10-15  \n",
      "55797                                  Brazil -- Fiction  2017-10-23  \n",
      "56737                 Rio de Janeiro (Brazil) -- Fiction  2018-03-14  \n",
      "57001                 Portuguese fiction -- 19th century  2018-04-19  \n",
      "67935                           Short stories, Brazilian  2022-04-26  \n",
      "33056                                            Fiction  2010-07-03  \n",
      "53101                                  Brazil -- Fiction  2016-09-20  \n",
      "67162                  Brazilian fiction -- 19th century  2022-01-14  \n",
      "67780                                  Brazil -- Fiction  2022-04-05  \n",
      "61653                                   Brazilian poetry  2020-03-21  \n"
     ]
    }
   ],
   "source": [
    "# create empty df to store the metadata\n",
    "df_html = pd.DataFrame(columns = ['author', 'title', 'lang', 'subj', 'datepub'])\n",
    "\n",
    "for book_id in book_id_list:\n",
    "    url_html = f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}-images.html'\n",
    "    data_html = download_url(url_html)\n",
    "\n",
    "    # parse\n",
    "    soup = BeautifulSoup(data_html, 'html.parser')\n",
    "\n",
    "    # get metadata\n",
    "    author = soup.find('meta', {'name' : 'AUTHOR'})['content'] if soup.find('meta', {'name' : 'AUTHOR'}) is not None else 'NA'\n",
    "    lang = soup.find('meta', {'name' : 'dc.language'})['content'] if soup.find('meta', {'name' : 'dc.language'}) is not None else 'NA'\n",
    "    subj = soup.find('meta', {'name' : 'dc.subject'})['content'] if soup.find('meta', {'name' : 'dc.subject'}) is not None else 'NA'\n",
    "    title = soup.find('meta', {'property' : 'og:title'})['content'] if soup.find('meta', {'property' : 'og:title'}) is not None else 'NA'\n",
    "    datepub = soup.find('meta', {'name' : 'dcterms.created'})['content'] if soup.find('meta', {'name' : 'dcterms.created'}) is not None else 'NA'\n",
    "\n",
    "    ## remove unnecessary elements\n",
    "    # style\n",
    "    for i in soup.find_all('style'):\n",
    "        i.decompose()\n",
    "\n",
    "    # boiler plates\n",
    "    for i in soup.find_all('section', {'class': re.compile('.*boilerplate.*')}):\n",
    "        i.decompose()\n",
    "\n",
    "    # editor comments\n",
    "    for i in soup.find_all('div', {'class': 'fbox'}):\n",
    "        i.decompose()\n",
    "\n",
    "    # page numbers\n",
    "    for i in soup.find_all('span', {'class': 'pagenum'}):\n",
    "        i.decompose()\n",
    "\n",
    "    # remove br tags\n",
    "    for i in soup.find_all('br'):\n",
    "        i.unwrap()\n",
    "\n",
    "    # remove head\n",
    "    soup.find('head').decompose()\n",
    "\n",
    "    # get metadata\n",
    "    df_html.loc[book_id] = [author, title, lang, subj, datepub]\n",
    "\n",
    "\n",
    "    # write to file with tags\n",
    "    with open(f'input/html/{book_id}.html', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(str(soup.prettify()))\n",
    "    # write to file without tags\n",
    "    with open(f'input/plain/{book_id}.txt', 'w', encoding = 'utf-8') as file:\n",
    "        file.write(soup.text)\n",
    "\n",
    "print(df_html)\n",
    "# write metadata to file\n",
    "df_html.to_csv('output/books_metadata.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@create_bow\n",
    "This script\n",
    "- reads plain text files in a give folder\n",
    "- applies Spacy Lang model\n",
    "- creates different bags of words ('all_tokens', 'full_clean', 'custom_tok')\n",
    "and saves:\n",
    "- the original book file (html)\n",
    "- the cleaned content (txt)\n",
    "- the metadata (tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load language model\n",
    "\n",
    "there are different models availables at https://spacy.io/models \n",
    "\n",
    "we can also create our own\n",
    "\n",
    "here we will use a small model to be more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get list with files\n",
    "the folder input has the plain files prepared with @get_gutemberg.py\n",
    "the function get_file_list creates a list and append the files names to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_list = utilsNLP.get_file_list('input/plain')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "list of elements to be removed (we can also here our own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags to be removed\n",
    "pos_rm = ['PUNCT', 'DET', 'SPACE', 'NUM', 'SYM']\n",
    "# Named Entities tags to be removed\n",
    "ner_rm = ['PER', 'LOC']\n",
    "# words to be removed\n",
    "wrd_rm = ['ella', 'elle']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go through the files extracting the words and save the bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty df to store the different bag of words (BoWs)\n",
    "df = pd.DataFrame(columns = ['all_tokens', 'full_clean', 'custom_tok'])\n",
    "\n",
    "# iterate each file and create the 3 different BoWs\n",
    "for val in file_list:\n",
    "    # read file\n",
    "    with open(val, 'r', encoding='utf-8') as f:\n",
    "        text_org = f.read()\n",
    "    \n",
    "    # remove line breaks\n",
    "    text_oneline = text_org.replace(\"\\n\", \" \")\n",
    "\n",
    "    # apply model\n",
    "    nlp_text = nlp(text_org)\n",
    "\n",
    "    # create a list to store the NER labes to be \n",
    "    ne2rm = []\n",
    "    for ent in nlp_text.ents:\n",
    "        if ent.label_ in ner_rm:\n",
    "            ne2rm.append(ent.text.lower())\n",
    "\n",
    "    # get lis of unique values for the ner found\n",
    "    ne2rm = list(set(ne2rm))\n",
    "\n",
    "    # other possibilities\n",
    "    # - remove numbers, but not words that contain numbers...\n",
    "    # - Remove words that are only one character...\n",
    "\n",
    "    # all tokens (no space)\n",
    "    print(f'getting all tokens BoW for {val.stem}...')\n",
    "    all_tokens = [token.text.lower() for token in nlp_text if token.pos_ != 'SPACE']\n",
    "\n",
    "    # get all lemma that are not in the removel list neither in the stop list and that is alpha (not letters)\n",
    "    print(\"getting BoW with a 'full clean' approach ...\")\n",
    "    full_clean = [token.lemma_.lower() for token in nlp_text if token.pos_ not in pos_rm and not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # remove locations and named person/family\n",
    "    print(\"getting customized BoW\")\n",
    "    custom_tok = [token.text.lower() for token in nlp_text if token.text.lower() not in ne2rm and token.text.lower() not in wrd_rm and token.pos_ not in pos_rm and not token.is_stop]\n",
    "\n",
    "    # add BoWs to dataframe\n",
    "    df.loc[val.stem] = [all_tokens, full_clean, custom_tok]\n",
    "\n",
    "# write dataframe to file\n",
    "df.to_csv('output/bows.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# print df \n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute bigrams.\n",
    "as this can be a very heavy (and slow) process, we make it separately \n",
    "and save it in a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get only the values from the all_tokens column\n",
    "bow = df['all_tokens']\n",
    "\n",
    "len(bow[0]) # 93208\n",
    "len(df['all_tokens'][0]) # 1369489\n",
    "\n",
    "# get bigrams that occur at least 5 times\n",
    "bigrams = Phrases(bow, min_count=5)\n",
    "\n",
    "# add bigrams to BoW\n",
    "for idx in range(len(bow)):\n",
    "    for token in bigrams[bow[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            bow[idx].append(token)\n",
    "\n",
    "# save to file\n",
    "bow.to_csv('output/bow_with2gram.tsv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cf00dd28cb843c250837a6461db654c95fdf5387681229a6a101a82db5d7e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
